{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1dda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://endpoint/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae94d16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Папка с .txt транскриптами\n",
    "TRANSCRIPTS_DIR = Path(\"/path/to/transcripts_dir\")\n",
    "\n",
    "# YAML с критериями/промптами\n",
    "PROMPTS_YAML_PATH = Path(\"/path/to/prompts.yaml\")\n",
    "\n",
    "# Куда сохранить результат\n",
    "OUTPUT_CSV_PATH = Path(\"/path/to/output.csv\")\n",
    "\n",
    "# Модель\n",
    "LLM_MODEL = \"gpt-oss_20b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79443994",
   "metadata": {},
   "source": [
    "# Загружаем критерии и транскрипты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88309bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_criteria_from_yaml(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "\n",
    "    raw_criteria = data[\"criteria\"]\n",
    "\n",
    "    criteria = []\n",
    "    for c in raw_criteria:\n",
    "        criteria.append({\n",
    "            \"id\": c.get(\"id\"),\n",
    "            \"name\": c.get(\"name\"),\n",
    "            \"description\": c.get(\"description\"),\n",
    "            \"prompt\": c.get(\"prompt\")\n",
    "        })\n",
    "    return criteria\n",
    "\n",
    "\n",
    "criteria = load_criteria_from_yaml(PROMPTS_YAML_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcripts_from_dir(dir_path: Path):\n",
    "    transcripts = []\n",
    "\n",
    "    for path in sorted(dir_path.glob(\"*.txt\")):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        transcripts.append({\n",
    "            \"file_name\": path.name,\n",
    "            \"path\": str(path),\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "    return transcripts\n",
    "\n",
    "transcripts = load_transcripts_from_dir(TRANSCRIPTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c920984",
   "metadata": {},
   "source": [
    "# Подготовка к работе с LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Ты — эксперт по контролю качества работы операторов контакт-центра.\n",
    "Твоя задача — оценивать, насколько оператор выполняет заданный критерий.\n",
    "\n",
    "Оцени по шкале:\n",
    "0 — критерий не выполнен (плохо)\n",
    "1 — критерий выполнен частично\n",
    "2 — критерий выполнен полностью\n",
    "\n",
    "Отвечай строго в формате JSON с полями:\n",
    "{\n",
    "  \"score\": 0/1/2,\n",
    "  \"explanation\": \"краткое объяснение на русском\"\n",
    "}\n",
    "Без лишнего текста до или после JSON.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_user_prompt(conversation_text: str, criterion: dict) -> str:\n",
    "    \"\"\"\n",
    "    Строим текст запроса к модели:\n",
    "    - описание критерия\n",
    "    - сам промпт критерия\n",
    "    - сам диалог\n",
    "    \"\"\"\n",
    "    template = f\"\"\"\n",
    "Критерий: {criterion.get(\"name\")}\n",
    "\n",
    "Описание критерия:\n",
    "{criterion.get(\"description\")}\n",
    "\n",
    "Инструкция для оценки:\n",
    "{criterion.get(\"prompt\")}\n",
    "\n",
    "Транскрипт разговора (Оператор/Клиент):\n",
    "\\\"\\\"\\\"text\n",
    "{conversation_text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Оцени, насколько оператор удовлетворяет этому критерию.\n",
    "Помни, нужно вернуть только JSON.\n",
    "\"\"\"\n",
    "    return template.strip()\n",
    "\n",
    "\n",
    "def evaluate_conversation_with_criterion(conversation_text: str, criterion: dict) -> dict:\n",
    "    user_prompt = build_user_prompt(conversation_text, criterion)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    raw_content = response.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        result = json.loads(raw_content)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"Невозможно распарсить JSON из ответа модели:\\n{raw_content}\")\n",
    "\n",
    "    score = int(result.get(\"score\"))\n",
    "    if score not in (0, 1, 2):\n",
    "        raise ValueError(f\"Неверное значение score: {score}, ответ модели: {result}\")\n",
    "\n",
    "    explanation = result.get(\"explanation\", \"\").strip()\n",
    "    return {\"score\": score, \"explanation\": explanation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5cb3b",
   "metadata": {},
   "source": [
    "# Прогон бейзлайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ff67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for t in tqdm(transcripts, desc=\"Transcripts\"):\n",
    "    conv_text = t[\"text\"]\n",
    "    file_name = t[\"file_name\"]\n",
    "\n",
    "    for criterion in criteria:\n",
    "        criterion_id = criterion.get(\"id\")\n",
    "        criterion_name = criterion.get(\"name\")\n",
    "\n",
    "        try:\n",
    "            eval_result = evaluate_conversation_with_criterion(conv_text, criterion)\n",
    "        except Exception as e:\n",
    "            eval_result = {\n",
    "                \"score\": None,\n",
    "                \"explanation\": f\"Ошибка оценки: {e}\"\n",
    "            }\n",
    "\n",
    "        results.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"criterion_id\": criterion_id,\n",
    "            \"criterion_name\": criterion_name,\n",
    "            \"score\": eval_result[\"score\"],\n",
    "            \"explanation\": eval_result[\"explanation\"],\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007146c",
   "metadata": {},
   "source": [
    "# Загружаем человеческую разметку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd92aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_LABELS_PATH = \"/path/to/human_labels.csv\"\n",
    "\n",
    "human_df = pd.read_csv(HUMAN_LABELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = (\n",
    "    df.merge(\n",
    "        human_df,\n",
    "        on=[\"file_name\", \"criterion_id\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_llm\", \"_human\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec38ed",
   "metadata": {},
   "source": [
    "# Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, weighted=False):\n",
    "    \"\"\"\n",
    "    weighted=False  -> обычные метрики\n",
    "    weighted=True   -> квадратически-взвешенная Kappa\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    if weighted:\n",
    "        kappa = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "    else:\n",
    "        kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "    return acc, kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb9ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_metrics = []\n",
    "\n",
    "for crit_id, g in merged.groupby(\"criterion_id\"):\n",
    "    y_true = g[\"human_label\"]\n",
    "    y_pred = g[\"score\"]\n",
    "\n",
    "    acc, kappa = compute_metrics(y_true, y_pred, weighted=True)\n",
    "\n",
    "    criterion_metrics.append({\n",
    "        \"criterion_id\": crit_id,\n",
    "        \"criterion_name\": g[\"criterion_name\"].iloc[0],\n",
    "        \"n_samples\": len(g),\n",
    "        \"accuracy\": acc,\n",
    "        \"cohen_kappa_weighted\": kappa,\n",
    "    })\n",
    "\n",
    "criterion_metrics_df = pd.DataFrame(criterion_metrics).sort_values(\"criterion_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "172b28c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.688, 0.512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_n = merged.shape[0]\n",
    "\n",
    "criterion_metrics_df[\"weight\"] = (\n",
    "    criterion_metrics_df[\"n_samples\"] / total_n\n",
    ")\n",
    "\n",
    "weighted_accuracy = round(np.sum(\n",
    "    criterion_metrics_df[\"accuracy\"] * criterion_metrics_df[\"weight\"]\n",
    "), 3)\n",
    "\n",
    "weighted_kappa = round(np.sum(\n",
    "    criterion_metrics_df[\"cohen_kappa_weighted\"] * criterion_metrics_df[\"weight\"]\n",
    "), 3)\n",
    "\n",
    "weighted_accuracy, weighted_kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e96a1b",
   "metadata": {},
   "source": [
    "# Прогон fine-tuned модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80f6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML с новыми промптами (few-shot)\n",
    "FINETUNE_PROMPTS_YAML_PATH = Path(\"/path/to/finetune_prompts.yaml\")\n",
    "\n",
    "OUTPUT_CSV_FINETUNE_PATH = Path(\"/path/to/output_finetune.csv\")\n",
    "\n",
    "FINETUNE_MODEL_NAME = \"gpt-oss_20b-finetuned\"\n",
    "\n",
    "finetune_client = OpenAI(\n",
    "    base_url=\"https://endpoint/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c6eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_criteria = load_criteria_from_yaml(FINETUNE_PROMPTS_YAML_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_finetune_messages(conversation_text: str, criterion: dict):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n",
    "    ]\n",
    "\n",
    "    for ex in criterion.get(\"few_shots\", []):\n",
    "        ex_conv = ex.get(\"conversation\", \"\")\n",
    "        ex_answer = ex.get(\"answer_json\", \"\").strip()\n",
    "\n",
    "        ex_user = f\"\"\"\n",
    "Критерий: {criterion.get(\"name\")}\n",
    "\n",
    "Описание критерия:\n",
    "{criterion.get(\"description\")}\n",
    "\n",
    "Инструкция:\n",
    "{criterion.get(\"prompt\")}\n",
    "\n",
    "Транскрипт разговора:\n",
    "\\\"\\\"\\\"text\n",
    "{ex_conv}\n",
    "\\\"\\\"\\\"\n",
    "Оцени, насколько оператор выполняет критерий. Ответь JSON.\n",
    "\"\"\".strip()\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": ex_user})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ex_answer})\n",
    "\n",
    "    final_user = f\"\"\"\n",
    "Критерий: {criterion.get(\"name\")}\n",
    "\n",
    "Описание критерия:\n",
    "{criterion.get(\"description\")}\n",
    "\n",
    "Инструкция:\n",
    "{criterion.get(\"prompt\")}\n",
    "\n",
    "Транскрипт разговора:\n",
    "\\\"\\\"\\\"text\n",
    "{conversation_text}\n",
    "\\\"\\\"\\\"\n",
    "Оцени, насколько оператор выполняет критерий. Ответь JSON.\n",
    "\"\"\".strip()\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": final_user})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def evaluate_conversation_with_criterion_finetune(conversation_text: str, criterion: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Вызывает fine-tuned модель и возвращает:\n",
    "    {\n",
    "        \"score\": 0/1/2 или None,\n",
    "        \"explanation\": str\n",
    "    }\n",
    "    \"\"\"\n",
    "    messages = build_finetune_messages(conversation_text, criterion)\n",
    "\n",
    "    response = finetune_client.chat.completions.create(\n",
    "        model=FINETUNE_MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    raw_content = response.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        result = json.loads(raw_content)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"[finetune] Невозможно распарсить JSON:\\n{raw_content}\")\n",
    "\n",
    "    score = int(result.get(\"score\"))\n",
    "    if score not in (0, 1, 2):\n",
    "        raise ValueError(f\"[finetune] Неверный score: {score}, ответ: {result}\")\n",
    "\n",
    "    explanation = result.get(\"explanation\", \"\").strip()\n",
    "    return {\"score\": score, \"explanation\": explanation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_results = []\n",
    "\n",
    "for t in tqdm(transcripts, desc=\"Transcripts (finetune)\"):\n",
    "    conv_text = t[\"text\"]\n",
    "    file_name = t[\"file_name\"]\n",
    "\n",
    "    for criterion in finetune_criteria:\n",
    "        criterion_id = criterion.get(\"id\")\n",
    "        criterion_name = criterion.get(\"name\")\n",
    "\n",
    "        try:\n",
    "            eval_result = evaluate_conversation_with_criterion_finetune(conv_text, criterion)\n",
    "        except Exception as e:\n",
    "            eval_result = {\n",
    "                \"score\": None,\n",
    "                \"explanation\": f\"Ошибка оценки (finetune): {e}\"\n",
    "            }\n",
    "\n",
    "        finetune_results.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"criterion_id\": criterion_id,\n",
    "            \"criterion_name\": criterion_name,\n",
    "            \"score_finetune\": eval_result[\"score\"],\n",
    "            \"explanation_finetune\": eval_result[\"explanation\"],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finetune = pd.DataFrame(finetune_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_finetune = (\n",
    "    df_finetune.merge(\n",
    "        human_df,\n",
    "        on=[\"file_name\", \"criterion_id\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b1be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_metrics_finetune = []\n",
    "\n",
    "for crit_id, g in merged_finetune.groupby(\"criterion_id\"):\n",
    "    y_true = g[\"human_label\"]\n",
    "    y_pred = g[\"score_finetune\"]\n",
    "\n",
    "    acc, kappa = compute_metrics(y_true, y_pred, weighted=True)\n",
    "\n",
    "    criterion_metrics_finetune.append({\n",
    "        \"criterion_id\": crit_id,\n",
    "        \"criterion_name\": g[\"criterion_name\"].iloc[0],\n",
    "        \"n_samples\": len(g),\n",
    "        \"accuracy_finetune\": acc,\n",
    "        \"cohen_kappa_weighted_finetune\": kappa,\n",
    "    })\n",
    "\n",
    "criterion_metrics_finetune_df = (\n",
    "    pd.DataFrame(criterion_metrics_finetune)\n",
    "    .sort_values(\"criterion_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdb69acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.873, 0.809)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_n_finetune = merged_finetune.shape[0]\n",
    "criterion_metrics_finetune_df[\"weight\"] = (\n",
    "    criterion_metrics_finetune_df[\"n_samples\"] / total_n_finetune\n",
    ")\n",
    "\n",
    "weighted_accuracy_finetune = np.sum(\n",
    "    criterion_metrics_finetune_df[\"accuracy_finetune\"] * criterion_metrics_finetune_df[\"weight\"]\n",
    ")\n",
    "\n",
    "weighted_kappa_finetune = np.sum(\n",
    "    criterion_metrics_finetune_df[\"cohen_kappa_weighted_finetune\"] * criterion_metrics_finetune_df[\"weight\"]\n",
    ")\n",
    "\n",
    "weighted_accuracy_finetune, weighted_kappa_finetune"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
